{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/awf/awf-misc/blob/main/FX_Experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Union[arg, ...]: each arg must be a type. Got Ellipsis.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [87], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfx_shnty\u001b[39;00m \u001b[39mimport\u001b[39;00m shnty_trace, ShapeAndType\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfx_print\u001b[39;00m \u001b[39mimport\u001b[39;00m fx_print\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maux\u001b[39m(p, q):\n",
            "File \u001b[0;32m~/dev/fx-tools/fx_shnty.py:74\u001b[0m\n\u001b[1;32m     71\u001b[0m ModuleSpec \u001b[39m=\u001b[39m \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m  \u001b[39m# TODO: modules\u001b[39;00m\n\u001b[1;32m     72\u001b[0m ShapePropagator \u001b[39m=\u001b[39m Callable[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, ShapeAndType]\n\u001b[0;32m---> 74\u001b[0m shnty_propagate: Dict[Union[FunctionSpec, MethodSpec, ModuleSpec], ShapePropagator] \u001b[39m=\u001b[39m {}\n\u001b[1;32m     77\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mShapeAndTypeTracer\u001b[39;00m(tfx\u001b[39m.\u001b[39mTracer):\n\u001b[1;32m     78\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, arg_shntys):\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/typing.py:312\u001b[0m, in \u001b[0;36m_tp_cache.<locals>.decorator.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# All real errors (not unhashable args) are raised below.\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/typing.py:403\u001b[0m, in \u001b[0;36m_SpecialForm.__getitem__\u001b[0;34m(self, parameters)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39m@_tp_cache\u001b[39m\n\u001b[1;32m    402\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, parameters):\n\u001b[0;32m--> 403\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(\u001b[39mself\u001b[39;49m, parameters)\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/typing.py:515\u001b[0m, in \u001b[0;36mUnion\u001b[0;34m(self, parameters)\u001b[0m\n\u001b[1;32m    513\u001b[0m     parameters \u001b[39m=\u001b[39m (parameters,)\n\u001b[1;32m    514\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mUnion[arg, ...]: each arg must be a type.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 515\u001b[0m parameters \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39;49m(_type_check(p, msg) \u001b[39mfor\u001b[39;49;00m p \u001b[39min\u001b[39;49;00m parameters)\n\u001b[1;32m    516\u001b[0m parameters \u001b[39m=\u001b[39m _remove_dups_flatten(parameters)\n\u001b[1;32m    517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(parameters) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/typing.py:515\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    513\u001b[0m     parameters \u001b[39m=\u001b[39m (parameters,)\n\u001b[1;32m    514\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mUnion[arg, ...]: each arg must be a type.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 515\u001b[0m parameters \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(_type_check(p, msg) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m parameters)\n\u001b[1;32m    516\u001b[0m parameters \u001b[39m=\u001b[39m _remove_dups_flatten(parameters)\n\u001b[1;32m    517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(parameters) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/typing.py:176\u001b[0m, in \u001b[0;36m_type_check\u001b[0;34m(arg, msg, is_argument, module, allow_special_forms)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[39mreturn\u001b[39;00m arg\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(arg):\n\u001b[0;32m--> 176\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m Got \u001b[39m\u001b[39m{\u001b[39;00marg\u001b[39m!r:\u001b[39;00m\u001b[39m.100\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    177\u001b[0m \u001b[39mreturn\u001b[39;00m arg\n",
            "\u001b[0;31mTypeError\u001b[0m: Union[arg, ...]: each arg must be a type. Got Ellipsis."
          ]
        }
      ],
      "source": [
        "from fx_shnty import shnty_trace, ShapeAndType\n",
        "from fx_print import fx_print\n",
        "\n",
        "def aux(p, q):\n",
        "  return torch.relu(1.234 * p * q).neg()\n",
        "\n",
        "def my_func(x, b):\n",
        "  y = 2 * x\n",
        "  for _ in range(2):  # Loops will be unrolled\n",
        "    x = aux(x, y)  # Function calls will be inlined\n",
        "  return torch.atan2(b, x)\n",
        "\n",
        "gm = shnty_trace(my_func, arg_shntys=(ShapeAndType((3, 5), torch.Tensor),))\n",
        "\n",
        "fx_print(gm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def my_func(x,b):\n",
            "  v10 = x\n",
            "  v11 = b\n",
            "  v12 = mul(2,v10)\n",
            "  v13 = mul(1.234,v10)\n",
            "  v14 = mul(v13,v12)\n",
            "  v15 = relu(v14)\n",
            "  v16 = v15.neg()\n",
            "  v17 = mul(1.234,v16)\n",
            "  v18 = mul(v17,v12)\n",
            "  v19 = relu(v18)\n",
            "  v20 = v19.neg()\n",
            "  v21 = atan2(v11,v20)\n",
            "  return v21\n"
          ]
        }
      ],
      "source": [
        "def fx_print(gm):\n",
        "  def commajoin(vs): return \",\".join(vs)\n",
        "  name2ord = {}\n",
        "  ord = 10\n",
        "  \n",
        "  \n",
        "  args = [n.name for n in gm.graph.nodes if n.op == 'placeholder']\n",
        "  name = torch.nn.Module._get_name(gm)\n",
        "  print(f'def {name}({commajoin(args)}):')\n",
        "  for n in gm.graph.nodes:\n",
        "      assert n.name not in name2ord\n",
        "      name2ord[n.name] = f'v{ord}'\n",
        "      ord += 1\n",
        "      target = n.target.__name__ if n.op == 'call_function' else n.target\n",
        "      def argstr(a):\n",
        "        if isinstance(a, tfx.Node):\n",
        "          return name2ord[a.name]\n",
        "        return str(a)\n",
        "      args = [argstr(a) for a in n.args]\n",
        "      pr = lambda x: print('  ' + x)\n",
        "      if n.op == 'placeholder':\n",
        "        pr(f'{argstr(n)} = {target}')\n",
        "      elif n.op == 'call_function':\n",
        "        pr(f'{argstr(n)} = {target}({commajoin(args)})')\n",
        "      elif n.op == 'call_method':\n",
        "        pr(f'{argstr(n)} = {argstr(args[0])}.{target}({\",\".join(args[1:])})')\n",
        "      elif n.op == 'output':\n",
        "        pr(f'return {argstr(args[0])}')\n",
        "      else:\n",
        "        pr(f'# unhandled {n.op} {argstr(n)} = {target}({\",\".join(args)})')\n",
        "      \n",
        "fx_print(gm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgQmrHBJDi2b",
        "outputId": "75caa67e-8d0c-42f0-8494-e2debc9294a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "def forward(self, x, b):\n",
            "    mul = 2 * x\n",
            "    mul_1 = 1.234 * x;  x = None\n",
            "    mul_2 = mul_1 * mul;  mul_1 = None\n",
            "    relu = torch.relu(mul_2);  mul_2 = None\n",
            "    neg = relu.neg();  relu = None\n",
            "    mul_3 = 1.234 * neg;  neg = None\n",
            "    mul_4 = mul_3 * mul;  mul_3 = mul = None\n",
            "    relu_1 = torch.relu(mul_4);  mul_4 = None\n",
            "    neg_1 = relu_1.neg();  relu_1 = None\n",
            "    atan2 = torch.atan2(b, neg_1);  b = neg_1 = None\n",
            "    return atan2\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "import torch, torch.fx\n",
        "import copy\n",
        "\n",
        "def aux(p,q):\n",
        "    return torch.relu(1.234*p*q).neg()\n",
        "\n",
        "def my_func(x, b):\n",
        "    y = 2 * x\n",
        "    for _ in range(2): # Loops will be unrolled\n",
        "        x = aux(x,y) # Function calls will be inlined\n",
        "    return torch.atan2(b,x)\n",
        "\n",
        "my_func_trace = torch.fx.symbolic_trace(my_func)\n",
        "print(my_func_trace.code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example: convert relu to gelu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oaclgwPxDnvh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "def forward(self, x, b):\n",
            "    mul = 2 * x\n",
            "    mul_1 = 1.234 * x;  x = None\n",
            "    mul_2 = mul_1 * mul;  mul_1 = None\n",
            "    relu = torch._C._nn.gelu(mul_2);  mul_2 = None\n",
            "    neg = relu.neg();  relu = None\n",
            "    mul_3 = 1.234 * neg;  neg = None\n",
            "    mul_4 = mul_3 * mul;  mul_3 = mul = None\n",
            "    relu_1 = torch._C._nn.gelu(mul_4);  mul_4 = None\n",
            "    neg_1 = relu_1.neg();  relu_1 = None\n",
            "    atan2 = torch.atan2(b, neg_1);  b = neg_1 = None\n",
            "    return atan2\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "def relu_to_gelu(mod: torch.fx.GraphModule):\n",
        "    g = mod.graph\n",
        "    for n in g.nodes:\n",
        "        if n.op == 'call_function' and n.target == torch.relu:\n",
        "            n.target = torch.nn.functional.gelu\n",
        "\n",
        "    mod.recompile()\n",
        "    return None # in-place modification of the graph\n",
        "\n",
        "my_func_trace = torch.fx.symbolic_trace(my_func)\n",
        "relu_to_gelu(my_func_trace)\n",
        "print(my_func_trace.code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Methods to functions\n",
        "\n",
        "Why _do_ we distinguish methods and functions?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "doing nothing to x\n",
            "doing nothing to b\n",
            "doing nothing to mul\n",
            "doing nothing to mul_1\n",
            "doing nothing to mul_2\n",
            "doing nothing to relu\n",
            "doing nothing to neg_2\n",
            "doing nothing to mul_3\n",
            "doing nothing to mul_4\n",
            "doing nothing to relu_1\n",
            "doing nothing to neg_3\n",
            "doing nothing to atan2\n",
            "doing nothing to output\n",
            "\n",
            "\n",
            "\n",
            "def forward(self, x, b):\n",
            "    mul = 2 * x\n",
            "    mul_1 = 1.234 * x;  x = None\n",
            "    mul_2 = mul_1 * mul;  mul_1 = None\n",
            "    relu = torch.relu(mul_2);  mul_2 = None\n",
            "    neg_2 = torch.neg(relu);  relu = None\n",
            "    mul_3 = 1.234 * neg_2;  neg_2 = None\n",
            "    mul_4 = mul_3 * mul;  mul_3 = mul = None\n",
            "    relu_1 = torch.relu(mul_4);  mul_4 = None\n",
            "    neg_3 = torch.neg(relu_1);  relu_1 = None\n",
            "    atan2 = torch.atan2(b, neg_3);  b = neg_3 = None\n",
            "    return atan2\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "# map from method name to function name\n",
        "def method_to_function(mod: torch.fx.GraphModule):\n",
        "    g = mod.graph\n",
        "    for n in g.nodes:\n",
        "        if n.op == 'call_method':\n",
        "            # create IR to call new activate\n",
        "            with g.inserting_after(n):\n",
        "                new_n = g.call_function(fn[n.target], n.args)\n",
        "                n.replace_all_uses_with(new_n)\n",
        "                g.erase_node(n)\n",
        "        else:\n",
        "            print('doing nothing to', n)\n",
        "\n",
        "    mod.recompile()\n",
        "    return None # in-place modification of the graph\n",
        "\n",
        "my_func_trace = torch.fx.symbolic_trace(my_func)\n",
        "method_to_function(my_func_trace)\n",
        "print(my_func_trace.code)\n",
        "\n",
        "# Look for:\n",
        "#   neg_2 = torch.neg(relu);"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMk6IojwKPOoEtWLs82Civ+",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 ('pytorch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "28f0c4cf9ff11e7fed1023153201d84a0ac9765962dc65ba2242229f995562f8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
