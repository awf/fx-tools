{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some experiments with DiffFX\n",
    "\n",
    "Some explorations of difffx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VJPs match OK\n",
      "def vjp_template(x,dret):\n",
      "  v10 = x\u001b[32m # Tensor[3x3, torch.float32]\u001b[0m\n",
      "  v11 = dret\u001b[32m # Tensor[3x3, torch.float32]\u001b[0m\n",
      "  v12 = torch.trace(v10)\u001b[32m # Tensor[(), torch.float32]\u001b[0m\n",
      "  v13 = torch.sin(v12)\u001b[32m # Tensor[(), torch.float32]\u001b[0m\n",
      "  v14 = mul(v13,v10)\u001b[32m # Tensor[3x3, torch.float32]\u001b[0m\n",
      "  v15 = mul(v10,v11)\u001b[32m # Tensor[3x3, torch.float32]\u001b[0m\n",
      "  v16 = mul(v13,v11)\u001b[32m # Tensor[3x3, torch.float32]\u001b[0m\n",
      "  v17 = v15.sum((0,1))\u001b[32m # Tensor[(), torch.float32]\u001b[0m\n",
      "  v18 = v17.reshape(())\u001b[32m # Tensor[(), torch.float32]\u001b[0m\n",
      "  v19 = v16.reshape((3,3))\u001b[32m # Tensor[3x3, torch.float32]\u001b[0m\n",
      "  v20 = add(0,v18)\u001b[32m # Tensor[(), torch.float32]\u001b[0m\n",
      "  v21 = add(0,v19)\u001b[32m # Tensor[3x3, torch.float32]\u001b[0m\n",
      "  v22 = torch.cos(v12)\u001b[32m # Tensor[(), torch.float32]\u001b[0m\n",
      "  v23 = mul(v22,v20)\u001b[32m # Tensor[(), torch.float32]\u001b[0m\n",
      "  v24 = add(0,v23)\u001b[32m # Tensor[(), torch.float32]\u001b[0m\n",
      "  v25 = self._tensor_constant0\u001b[32m # Tensor[3x3, torch.float32]f32[3x3] [[1.000 0.000 0.000], [0.000 1.000 0.000], [0.000 0.000 1.000]]\u001b[0m\n",
      "  v26 = mul(v24,v25)\u001b[32m # Tensor[3x3, torch.float32]\u001b[0m\n",
      "  v27 = add(v21,v26)\u001b[32m # Tensor[3x3, torch.float32]\u001b[0m\n",
      "  return (v14,v27)\u001b[32m # (Tensor[3x3, torch.float32],Tensor[3x3, torch.float32])\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from fx_print import fx_print\n",
    "from difffx import vjp as fx_vjp, fx_add_shapes\n",
    "import vjp_rules # TODO: don't require this\n",
    "from awfutils.pytree_utils import PyTree\n",
    "\n",
    "# Function to vjp\n",
    "def foo(x):\n",
    "    w = torch.trace(x)\n",
    "    w = torch.sin(w)\n",
    "    a = w * x\n",
    "    return a\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "x = torch.randn(3,3)\n",
    "foo_vjp = fx_vjp(foo, x)\n",
    "\n",
    "dret = torch.randn_like(foo(x))\n",
    "foo_vjp_pt = lambda x,dret: torch.autograd.functional.vjp(foo, x, dret)\n",
    "\n",
    "PyTree.assert_close(foo_vjp_pt(x,dret), foo_vjp(x, dret))\n",
    "print('VJPs match OK')\n",
    "\n",
    "fx_add_shapes(foo_vjp, (x, dret))\n",
    "fx_print(foo_vjp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VJPs match\n"
     ]
    }
   ],
   "source": [
    "# Manual VJP to compare to\n",
    "def foo_vjp_manual(x, dret):\n",
    "    w = torch.trace(x)\n",
    "    w1 = torch.sin(w)\n",
    "    ret = w1 * x\n",
    "    \n",
    "    dw1 = torch.sum(dret * x)\n",
    "    dx = w1 * dret\n",
    "\n",
    "    dw = torch.cos(w) * dw1\n",
    "\n",
    "    dx += dw * torch.eye(*x.shape)\n",
    "    \n",
    "    return ret, dx\n",
    "\n",
    "PyTree.assert_close(foo_vjp_manual(x, dret), foo_vjp_pt(x,dret))\n",
    "print('VJPs match')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class vjp_template(torch.nn.Module):\n",
      "    def forward(self, x: \"f32[3, 3]\", dret: \"f32[3, 3]\"):\n",
      "        # No stacktrace found for following nodes\n",
      "        trace: \"f32[]\" = torch.trace(x)\n",
      "        sin: \"f32[]\" = torch.sin(trace)\n",
      "        mul: \"f32[3, 3]\" = sin * x\n",
      "        mul_1: \"f32[3, 3]\" = x * dret;  x = None\n",
      "        mul_2: \"f32[3, 3]\" = sin * dret;  sin = dret = None\n",
      "        sum_1: \"f32[]\" = mul_1.sum((0, 1));  mul_1 = None\n",
      "        reshape: \"f32[]\" = sum_1.reshape(());  sum_1 = None\n",
      "        reshape_1: \"f32[3, 3]\" = mul_2.reshape((3, 3));  mul_2 = None\n",
      "        add: \"f32[]\" = 0 + reshape;  reshape = None\n",
      "        add_1: \"f32[3, 3]\" = 0 + reshape_1;  reshape_1 = None\n",
      "        cos: \"f32[]\" = torch.cos(trace);  trace = None\n",
      "        mul_3: \"f32[]\" = cos * add;  cos = add = None\n",
      "        add_2: \"f32[]\" = 0 + mul_3;  mul_3 = None\n",
      "        _tensor_constant0: \"f32[3, 3]\" = self._tensor_constant0\n",
      "        mul_4: \"f32[3, 3]\" = add_2 * _tensor_constant0;  add_2 = _tensor_constant0 = None\n",
      "        add_3: \"f32[3, 3]\" = add_1 + mul_4;  add_1 = mul_4 = None\n",
      "        return (mul, add_3)\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "foo_vjp.print_readable();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using FX IR to print source code of functorch.jacrev(f)\n",
    "\n",
    "This is rather more low-level than the AD above, as it reflects the operations that hit the torch dispatcher.\n",
    "This also means it is size-specialized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def f(x_1):\n",
      "  v10 = x_1\u001b[32m # Tensor[13, torch.float32],val:FakeTensor(..., size=(13,))\u001b[0m\n",
      "  v11 = aten.sin.default(v10)\u001b[32m # Tensor[13, torch.float32],val:FakeTensor(..., size=(13,))\u001b[0m\n",
      "  v12 = aten.add.Tensor(v11,v10)\u001b[32m # Tensor[13, torch.float32],val:FakeTensor(..., size=(13,))\u001b[0m\n",
      "  v13 = self._tensor_constant0\u001b[32m # i64[1] [13]\u001b[0m\n",
      "  v14 = aten.lift_fresh_copy.default(v13)\u001b[32m # Tensor[1, torch.int64],val:FakeTensor(..., size=(1,), dtype=torch.int64)\u001b[0m\n",
      "  v15 = aten.cumsum.default(v14,0)\u001b[32m # Tensor[1, torch.int64],val:FakeTensor(..., size=(1,), dtype=torch.int64)\u001b[0m\n",
      "  v16 = aten.slice.Tensor(v15,0,0,-1)\u001b[32m # Tensor[0, torch.int64],val:FakeTensor(..., size=(0,), dtype=torch.int64)\u001b[0m\n",
      "  v17 = aten.neg.default(v16)\u001b[32m # Tensor[0, torch.int64],val:FakeTensor(..., size=(0,), dtype=torch.int64)\u001b[0m\n",
      "  v18 = aten.unbind.int(v17)\u001b[32m # val:[]\u001b[0m\n",
      "  v19 = aten.new_zeros.default(v12,[13,13])\u001b[32m # Tensor[13x13, torch.float32],val:FakeTensor(..., size=(13, 13))\u001b[0m\n",
      "  v20 = aten.diagonal.default(v19)\u001b[32m # Tensor[13, torch.float32],val:FakeTensor(..., size=(13,))\u001b[0m\n",
      "  v21 = aten.fill_.Scalar(v20,1)\u001b[32m # Tensor[13, torch.float32],val:FakeTensor(..., size=(13,))\u001b[0m\n",
      "  v22 = aten.view.default(v19,[13,13])\u001b[32m # Tensor[13x13, torch.float32],val:FakeTensor(..., size=(13, 13))\u001b[0m\n",
      "  v23 = aten.cos.default(v10)\u001b[32m # Tensor[13, torch.float32],val:FakeTensor(..., size=(13,))\u001b[0m\n",
      "  v24 = aten.mul.Tensor(v22,v23)\u001b[32m # Tensor[13x13, torch.float32],val:FakeTensor(..., size=(13, 13))\u001b[0m\n",
      "  v25 = aten.add.Tensor(v22,v24)\u001b[32m # Tensor[13x13, torch.float32],val:FakeTensor(..., size=(13, 13))\u001b[0m\n",
      "  v26 = aten.split_with_sizes.default(v25,[13])\u001b[32m # val:[FakeTensor(..., size=(13, 13))]\u001b[0m\n",
      "  v27 = v26[0]\u001b[32m # Tensor[13x13, torch.float32],val:FakeTensor(..., size=(13, 13))\u001b[0m\n",
      "  v28 = aten.view.default(v27,[13,13])\u001b[32m # Tensor[13x13, torch.float32],val:FakeTensor(..., size=(13, 13))\u001b[0m\n",
      "  return v28\n"
     ]
    }
   ],
   "source": [
    "from functorch import make_fx\n",
    "def f(x):\n",
    "    return torch.sin(x) + x\n",
    "x = torch.randn(13)\n",
    "grad_f = make_fx(torch.func.jacrev(f))(x)\n",
    "fx_print(grad_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, the FX AD version is closer to what one might write by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def vjp_template(x,dret):\n",
      "  v10 = x\n",
      "  v11 = dret\n",
      "  v12 = torch.sin(v10)\n",
      "  v13 = add(v12,v10)\n",
      "  v14 = v11.reshape((13))\n",
      "  v15 = v11.reshape((13))\n",
      "  v16 = add(0,v14)\n",
      "  v17 = add(0,v15)\n",
      "  v18 = torch.cos(v10)\n",
      "  v19 = mul(v18,v16)\n",
      "  v20 = add(v17,v19)\n",
      "  return (v13,v20)\n"
     ]
    }
   ],
   "source": [
    "fx_print(fx_vjp(f, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMk6IojwKPOoEtWLs82Civ+",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "awfutils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
