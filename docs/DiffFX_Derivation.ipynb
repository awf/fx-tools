{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiffFX Derivation\n",
    "\n",
    "This notebook is a walkthrough of the initial design and implementation of difffx,\n",
    "a simple source-to-source autodiff tool using Torch FX.\n",
    "See https://github.com/pytorch/pytorch/blob/master/torch/fx/OVERVIEW.md#the-fx-ir for\n",
    "an overview of the FX IR.\n",
    "\n",
    "See `difffx.py` for the actual implementation, which covers cases not shown here,\n",
    "particularly:\n",
    " - Dealing with shapes (e.g. the derivative of `trace` needs to know the argument size)\n",
    " - Dealing with method calls (this code just handles function calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import prerequisites, define utility functions\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.fx as fx\n",
    "\n",
    "from awfutils.pytree_utils import PyTree\n",
    "\n",
    "from vjp_check import vjp_check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FX_VJP: Source-to-source reverse mode algorithmic differentiation via FX \n",
    "\n",
    "An FX Interpreter that implements reverse-mode automatic differentiation.\n",
    "\n",
    "It may help to recall the basic laws of AD.\n",
    "Given function `f` which takes arbitrary pytree type `S`, and returns arbitary pytree `T`,\n",
    "we define the vector-Jacobian product `vjp{f}` which takes types `(S, dT)` and returns type `dS` with `vjp{f}(s, dt) = dt * J{f}(s)`.\n",
    "When `T` is a scalar or has only one element, then the VJP is the gradient,\n",
    "so `grad{f}(s) = vjp{f}(s, 1.0)`.\n",
    "```py\n",
    "def f(s : S) -> T: \n",
    "  ...\n",
    "def vjp{f}(s : S, dt : dT) -> dS: \n",
    "  ...\n",
    "```\n",
    "and as we generally divide the computation into\n",
    "'forward' and 'backward' passes, returning the result of `f` in the forward pass,\n",
    "as well as some auxiliary information of type `Aux{f}` \n",
    "```py\n",
    "def fwd{f}(s : S) -> (T, Aux{f}): \n",
    "  ...\n",
    "def bwd{f}(aux : Aux{f}, dt : dT) -> dS:\n",
    "  ...\n",
    "```\n",
    "in terms of which we could just write `vjp{f}` as\n",
    "```py\n",
    "def vjp{f}(s : S, dt : dT) -> dS:\n",
    "  _t, Aux = fwd{f}(s)\n",
    "  return bwd{f}(Aux, dt)\n",
    "```\n",
    "\n",
    "Here are examples for `add`, `mul`, and `matmul`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All asserts passed\n"
     ]
    }
   ],
   "source": [
    "# If they were scalars: d/da(a+b) = 1, d/db(a+b) = 1\n",
    "def add_fwd(a, b):\n",
    "    return a + b, None # Aux is empty\n",
    "\n",
    "def add_bwd(aux, dret):\n",
    "    return (dret, dret)\n",
    "\n",
    "# If they were scalars: d/da(a*b) = b, d/db(a*b) = a\n",
    "def mul_fwd(a, b):\n",
    "    return a * b, (a,b) # Aux needs to remember both a and b\n",
    "\n",
    "def mul_bwd(aux, dret):\n",
    "    a,b = aux # Unpack the inputs\n",
    "    return (dret*b, a*dret)\n",
    "\n",
    "# And for matmul, the sizes need to line up\n",
    "# MxK @ KxN -> MxN\n",
    "def matmul_fwd(a, b):\n",
    "    return a @ b, (a,b) # Aux needs to remember both a and b\n",
    "\n",
    "def matmul_bwd(aux, dret):\n",
    "    a,b = aux # Unpack the inputs\n",
    "    # dret is MxN, a is MxK, b is KxN\n",
    "    # dA should be MxK, dB should be KxN\n",
    "    da = dret @ b.t()\n",
    "    db = a.t() @ dret\n",
    "    return (da, db)\n",
    "\n",
    "\n",
    "## Just checking the above are correct\n",
    "from vjp_check import vjp_check_fwdbwd\n",
    "vjp_check_fwdbwd(torch.add, add_fwd, add_bwd, (torch.randn(3,2), torch.randn(3,2)))\n",
    "vjp_check_fwdbwd(torch.mul, mul_fwd, mul_bwd, (torch.randn(3,2), torch.randn(3,2)))\n",
    "vjp_check_fwdbwd(torch.mm, matmul_fwd, matmul_bwd, (torch.randn(3,2), torch.randn(2,5)))\n",
    "print('All asserts passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A choice of VJPs?\n",
    "\n",
    "Here's a more interesting example for `sin`, where we provide two implementations, \n",
    "one likely to be more memory efficient, the other likely to be faster.\n",
    "How to choose automatically between them is a question for another day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: sin\n",
    "OPTIMIZE_FOR_MEMORY = True\n",
    "if OPTIMIZE_FOR_MEMORY:\n",
    "    # sin saves x in Aux - may save memory as x is likely preserved for backward pass\n",
    "    def sin_fwd(x):\n",
    "        return (torch.sin(x), x)\n",
    "    def sin_bwd(aux_is_x,dret): \n",
    "        return torch.cos(aux_is_x) * dret\n",
    "else:\n",
    "    # sin saves cos(x) in Aux - may save compute if `sincos`` is faster than `sin` and `cos`\n",
    "    def sin_fwd(x):\n",
    "        ret, aux = torch.sincos(x)\n",
    "        return ret, aux\n",
    "    def sin_bwd(aux_is_cosx,dret): \n",
    "        return aux_is_cosx * dret\n",
    "\n",
    "vjp_check_fwdbwd(torch.sin, sin_fwd, sin_bwd, (torch.randn(3,2),))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain rule\n",
    "\n",
    "An FX traced function is of the general form\n",
    "```py\n",
    "def foo(a1..an):\n",
    "  t1 = f1(a1..an)\n",
    "  t2 =  f2(a1..an,t1) # wlog, fk uses all in-scope variables\n",
    "  ...\n",
    "  tm =     fm(a1..an,t1..t{m-1}) \n",
    "  return tm\n",
    "```\n",
    "Then the VJP (vector-jacobian product) is of the form\n",
    "```py\n",
    "def foo_vjp(a1..an, dtm):\n",
    "  # Forward pass\n",
    "  t1, aux1               = f1_fwd(a1..an)\n",
    "  t2, aux2               =   f2_fwd(a1..an,t1)\n",
    "  ...\n",
    "  tm, auxm               =      fm_fwd(a1..an,t1..t{m-1})\n",
    "\n",
    "  # Backward pass\n",
    "  da1..dan,dt1..dt{m-1} +=      fm_bwd(auxm, dtm)\n",
    "  ...\n",
    "  da{1..n},dt1          +=   f2_bwd(aux2, dt3)\n",
    "  da{1..n}              += f1_bwd(aux1, dt1)\n",
    "\n",
    "  return da{1..n}\n",
    "```\n",
    "\n",
    "So let's make a transformer `fx_vjp` that does that.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A mapping from python function to (forward, backward)\n",
    "ad_map = {}\n",
    "\n",
    "# Register the AD functions from above (lots more are done in vjp_rules.py)\n",
    "import operator\n",
    "ad_map[operator.add] = add_fwd, add_bwd\n",
    "ad_map[operator.mul] = mul_fwd, mul_bwd\n",
    "ad_map[operator.matmul] = matmul_fwd, matmul_bwd\n",
    "ad_map[torch.sin] = sin_fwd, sin_bwd\n",
    "\n",
    "def fx_vjp(f, sample_input):\n",
    "    \"\"\"\n",
    "    An FX transform that implements reverse-mode automatic differentiation.\n",
    "    \"\"\"\n",
    "\n",
    "    class ADInterpreter(torch.fx.Interpreter):\n",
    "        \"\"\"\n",
    "        This interpreter runs through the forward transformation, \n",
    "        replacing calls to `fk` with `fk_fwd = ad_map[fk][0]`,\n",
    "        and recording the operations on a stack.\n",
    "        \"\"\"\n",
    "        def __init__(self, f):\n",
    "            super().__init__(f)\n",
    "            self.stack = []\n",
    "\n",
    "        def call_function(self, target, args, kwargs):\n",
    "            assert kwargs == None or len(kwargs) == 0\n",
    "\n",
    "            if target not in ad_map:\n",
    "                raise NotImplementedError(f\"Need VJP rule for {target}\")\n",
    "            # Look up forward/backward functions in `ad_map`\n",
    "            fwd,bwd = ad_map[target]\n",
    "            # Call the fwd function, getting proxies for returns\n",
    "            val,aux = fwd(*args)\n",
    "            # In the backward pass, we will compute:\n",
    "            #  d[args[0]],...,d[args[-1]] = bwd(aux, d{val})\n",
    "            # So remember: (args, bwd, aux, val)\n",
    "            # Note that all of these are symbolic, so it's cheap to remember them\n",
    "            self.stack.append((args, bwd, aux, val))\n",
    "            # And return the return value (a proxy)\n",
    "            return val\n",
    "\n",
    "        def call_method(self, target, args, kwargs):\n",
    "            raise NotImplementedError # see difffx.py for how to implement this\n",
    "\n",
    "        def get_attr(self, target, args, kwargs):\n",
    "            raise NotImplementedError # see difffx.py for how to implement this\n",
    "\n",
    "    # Grab the FX graph\n",
    "    f_trace = torch.fx.symbolic_trace(f)\n",
    "    \n",
    "    # Run torch's shape analysis, record answers in the graph\n",
    "    torch.fx.passes.graph_manipulation.ShapeProp(f_trace).run(sample_input)\n",
    "\n",
    "    # This is the \"template\" function for the VJP - symbolically tracing this template\n",
    "    # will generate the VJP function.\n",
    "    def vjp_template(x, dret):\n",
    "        # Run the forward computations, and collect them in ad.stack\n",
    "        ad = ADInterpreter(f_trace)\n",
    "        ret = ad.run(x)\n",
    "        # Build a dict to hold derivatives\n",
    "        d =  defaultdict(lambda: 0)\n",
    "        # Add dret to derivatives dict\n",
    "        d[ret] = dret\n",
    "        # And run down the stack...\n",
    "        for (args, bwd, aux, val) in reversed(ad.stack):\n",
    "            dargs = bwd(aux, d[val])\n",
    "            for (a,da) in zip(args, dargs if isinstance(dargs, tuple) else (dargs,)):\n",
    "                d[a] += da\n",
    "        # And return ret and J'*dret\n",
    "        return ret, d[x]\n",
    "\n",
    "    # Trace through vjp_template and return.\n",
    "    return torch.fx.symbolic_trace(vjp_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... and let's try it out\n",
    "\n",
    "We define a function `foo`, compute its vjp using DiffFX, and compare to `autograd.functional.vjp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VJP matches PyTorch\n"
     ]
    }
   ],
   "source": [
    "# Function to vjp\n",
    "def foo(x):\n",
    "    w = torch.sin(x)\n",
    "    y = w * x + x\n",
    "    z = w * torch.sin(x)\n",
    "    return y + z\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "x = torch.randn(3,3)\n",
    "foo_vjp = fx_vjp(foo, x)\n",
    "\n",
    "dret = torch.randn_like(foo(x))\n",
    "foo_vjp_pt = lambda x,dret: torch.autograd.functional.vjp(foo, x, dret)\n",
    "\n",
    "PyTree.assert_close(foo_vjp(x,dret), foo_vjp_pt(x, dret))\n",
    "print('VJP matches PyTorch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's show that it's a real source-to-source transformation, by printing the code for the VJP.\n",
    "\n",
    "(Note there is nicer printing in `fx_print`, this is just to make this notebook more colabbable )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class vjp_template(torch.nn.Module):\n",
      "    def forward(self, x, dret):\n",
      "        # No stacktrace found for following nodes\n",
      "        sin = torch.sin(x)\n",
      "        mul = sin * x\n",
      "        add = mul + x;  mul = None\n",
      "        sin_1 = torch.sin(x)\n",
      "        mul_1 = sin * sin_1\n",
      "        add_1 = add + mul_1;  add = mul_1 = None\n",
      "        add_2 = 0 + dret\n",
      "        add_3 = 0 + dret;  dret = None\n",
      "        mul_2 = add_3 * sin_1;  sin_1 = None\n",
      "        mul_3 = sin * add_3;  add_3 = None\n",
      "        add_4 = 0 + mul_2;  mul_2 = None\n",
      "        add_5 = 0 + mul_3;  mul_3 = None\n",
      "        cos = torch.cos(x)\n",
      "        mul_4 = cos * add_5;  cos = add_5 = None\n",
      "        add_6 = 0 + mul_4;  mul_4 = None\n",
      "        add_7 = 0 + add_2\n",
      "        add_8 = add_6 + add_2;  add_6 = add_2 = None\n",
      "        mul_5 = add_7 * x\n",
      "        mul_6 = sin * add_7;  sin = add_7 = None\n",
      "        add_9 = add_4 + mul_5;  add_4 = mul_5 = None\n",
      "        add_10 = add_8 + mul_6;  add_8 = mul_6 = None\n",
      "        cos_1 = torch.cos(x);  x = None\n",
      "        mul_7 = cos_1 * add_9;  cos_1 = add_9 = None\n",
      "        add_11 = add_10 + mul_7;  add_10 = mul_7 = None\n",
      "        return (add_1, add_11)\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "foo_vjp.print_readable();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to PyTorch's jacrev\n",
    "\n",
    "Here's the output from PyTorch's `jacrev`.  Because it is implemented at a \n",
    "slightly lower level, the output is arguably less easy to understand and to \n",
    "modify if that were one's goal.  On the other hand, PyTorch's implementation\n",
    "deals with huge numbers of cases that ours does not, emphasizing that this \n",
    "is primarily a pedagocical exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class foo(torch.nn.Module):\n",
      "    def forward(self, x_1: \"f32[3, 3]\"):\n",
      "        # No stacktrace found for following nodes\n",
      "        sin: \"f32[3, 3]\" = torch.ops.aten.sin.default(x_1)\n",
      "        mul: \"f32[3, 3]\" = torch.ops.aten.mul.Tensor(sin, x_1)\n",
      "        add: \"f32[3, 3]\" = torch.ops.aten.add.Tensor(mul, x_1);  mul = None\n",
      "        sin_1: \"f32[3, 3]\" = torch.ops.aten.sin.default(x_1)\n",
      "        mul_1: \"f32[3, 3]\" = torch.ops.aten.mul.Tensor(sin, sin_1)\n",
      "        add_1: \"f32[3, 3]\" = torch.ops.aten.add.Tensor(add, mul_1);  add = mul_1 = None\n",
      "        _tensor_constant0 = self._tensor_constant0\n",
      "        lift_fresh_copy: \"i64[1]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None\n",
      "        cumsum: \"i64[1]\" = torch.ops.aten.cumsum.default(lift_fresh_copy, 0);  lift_fresh_copy = None\n",
      "        slice_1: \"i64[0]\" = torch.ops.aten.slice.Tensor(cumsum, 0, 0, -1);  cumsum = None\n",
      "        neg: \"i64[0]\" = torch.ops.aten.neg.default(slice_1);  slice_1 = None\n",
      "        unbind = torch.ops.aten.unbind.int(neg);  neg = unbind = None\n",
      "        new_zeros: \"f32[9, 9]\" = torch.ops.aten.new_zeros.default(add_1, [9, 9], pin_memory = False);  add_1 = None\n",
      "        diagonal: \"f32[9]\" = torch.ops.aten.diagonal.default(new_zeros)\n",
      "        fill_: \"f32[9]\" = torch.ops.aten.fill_.Scalar(diagonal, 1);  diagonal = fill_ = None\n",
      "        view: \"f32[9, 3, 3]\" = torch.ops.aten.view.default(new_zeros, [9, 3, 3]);  new_zeros = None\n",
      "        mul_2: \"f32[9, 3, 3]\" = torch.ops.aten.mul.Tensor(view, sin)\n",
      "        mul_3: \"f32[9, 3, 3]\" = torch.ops.aten.mul.Tensor(view, sin_1);  sin_1 = None\n",
      "        cos: \"f32[3, 3]\" = torch.ops.aten.cos.default(x_1)\n",
      "        mul_4: \"f32[9, 3, 3]\" = torch.ops.aten.mul.Tensor(mul_2, cos);  mul_2 = cos = None\n",
      "        add_2: \"f32[9, 3, 3]\" = torch.ops.aten.add.Tensor(mul_4, view);  mul_4 = None\n",
      "        mul_5: \"f32[9, 3, 3]\" = torch.ops.aten.mul.Tensor(view, sin);  sin = None\n",
      "        mul_6: \"f32[9, 3, 3]\" = torch.ops.aten.mul.Tensor(view, x_1);  view = None\n",
      "        add_3: \"f32[9, 3, 3]\" = torch.ops.aten.add.Tensor(mul_3, mul_6);  mul_3 = mul_6 = None\n",
      "        add_4: \"f32[9, 3, 3]\" = torch.ops.aten.add.Tensor(add_2, mul_5);  add_2 = mul_5 = None\n",
      "        cos_1: \"f32[3, 3]\" = torch.ops.aten.cos.default(x_1);  x_1 = None\n",
      "        mul_7: \"f32[9, 3, 3]\" = torch.ops.aten.mul.Tensor(add_3, cos_1);  add_3 = cos_1 = None\n",
      "        add_5: \"f32[9, 3, 3]\" = torch.ops.aten.add.Tensor(add_4, mul_7);  add_4 = mul_7 = None\n",
      "        split_with_sizes = torch.ops.aten.split_with_sizes.default(add_5, [9]);  add_5 = None\n",
      "        getitem: \"f32[9, 3, 3]\" = split_with_sizes[0];  split_with_sizes = None\n",
      "        view_1: \"f32[3, 3, 3, 3]\" = torch.ops.aten.view.default(getitem, [3, 3, 3, 3]);  getitem = None\n",
      "        return view_1\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "import functorch\n",
    "\n",
    "grad_f = functorch.make_fx(torch.func.jacrev(foo))(x)\n",
    "grad_f.print_readable();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook shows the from-scratch derivation of an automatic differentiation\n",
    "transformation in torch FX.  The interested reader should look at `difffx.py` to \n",
    "see more details."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMk6IojwKPOoEtWLs82Civ+",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "awfutils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
