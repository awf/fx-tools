{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Some experiments with PyTorch FX.\n",
        "\n",
        "In which we make a simple source-to-source autodiff tool.\n",
        "See https://github.com/pytorch/pytorch/blob/master/torch/fx/OVERVIEW.md#the-fx-ir for\n",
        "an overview of the FX IR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Import prerequisites, define utility functions\n",
        "from collections import defaultdict\n",
        "\n",
        "import operator\n",
        "\n",
        "import torch\n",
        "import torch.fx as fx\n",
        "\n",
        "from icecream import ic \n",
        "\n",
        "from awfutils.pytree_utils import pt_rand_like, PyTree\n",
        "\n",
        "from vjp_check import vjp_check\n",
        "\n",
        "def ensure_tuple(x):\n",
        "    if isinstance(x, tuple):\n",
        "        return x\n",
        "    return (x,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Shape propagation\n",
        "\n",
        "This is an aside -- we need shapes for e.g. the gradient of 'trace', so let's\n",
        "quickly assemble a solution, where we can call `fx_shape` on proxies.\n",
        "\n",
        "Quick hack here, as we expect more thorough handling upstream:\n",
        " - https://discuss.pytorch.org/t/fx-proxy-and-shapes/113861/4\n",
        " - https://github.com/pytorch/pytorch/issues/54982\n",
        " - https://www.youtube.com/watch?v=pLni96jtcjY\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "import torch.fx.passes\n",
        "\n",
        "class AnnotatingInterpreter(torch.fx.Interpreter):\n",
        "    \"\"\"\n",
        "    An FX Interpreter that attaches the original FX node to proxies.\n",
        "\n",
        "    This allows annotations left by previous passes to be picked up, for example shapes\n",
        "    \"\"\"\n",
        "    def run_node(self, n):\n",
        "        val = super().run_node(n)\n",
        "        val.fxi_node = n # Attach node to val\n",
        "        return val\n",
        "\n",
        "def fx_add_shapes(f_trace : torch.fx.GraphModule, sample_input : Any):\n",
        "    \"\"\"\n",
        "    Run shape propagation on graph `f_trace`, which will add shape metadata in place.\n",
        "    \"\"\"\n",
        "    torch.fx.passes.graph_manipulation.ShapeProp(f_trace).run(sample_input)\n",
        "\n",
        "def fx_shape(x):\n",
        "    \"\"\"\n",
        "    Return the shape of FX Proxy x.\n",
        "\n",
        "    Assumes that ShapeProp has been run on the graph, so that x.fsi_node is set\n",
        "    \"\"\"\n",
        "    return x.fxi_node.meta['tensor_meta'].shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FX_VJP: Source-to-source reverse mode algorithmic differentiation via FX \n",
        "\n",
        "An FX Interpreter that implements reverse-mode automatic differentiation.\n",
        "\n",
        "It may help to recall the basic laws of AD.\n",
        "Given function `f` which takes arbitrary pytree type `S`, and returns arbitary pytree `T`,\n",
        "we define the vector-Jacobian product `vjp{f}` which takes types `(S, dT)` and returns type `dS` with `vjp{f}(s, dt) = dt * J{f}(s)`.\n",
        "When `T` is a scalar or has only one element, then the VJP is the gradient,\n",
        "so `grad{f}(s) = vjp{f}(s, 1.0)`.\n",
        "```py\n",
        "def f(s : S) -> T: ...\n",
        "def vjp{f}(s : S, dt : dT) -> dS: ...\n",
        "```\n",
        "and as we generally divide the computation into\n",
        "'forward' and 'backward' passes, returning the result of `f` in the forward pass,\n",
        "as well as some auxiliary information of type `Aux{f}` \n",
        "```py\n",
        "def fwd{f}(s : S) -> (T, Aux{f}): ...\n",
        "def bwd{f}(aux : Aux{f}, dt : dT) -> dS: ...\n",
        "```\n",
        "in terms of which we could just write `vjp{f}` as\n",
        "```py\n",
        "def vjp{f}(s : S, dt : dT) -> dS:\n",
        "  _t, Aux = fwd{f}(s)\n",
        "  return bwd{f}(Aux, dt)\n",
        "```\n",
        "\n",
        "Here's an example for `sin`, where we provide two implementations, \n",
        "one likely to be more memory efficient, the other likely to be faster. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: sin\n",
        "if True:\n",
        "    # sin saves x in Aux - may save memory as x is likely preserved for backward pass\n",
        "    def sin_fwd(x):\n",
        "        return (torch.sin(x), x)\n",
        "    def sin_bwd(aux_is_x,dret): \n",
        "        return torch.cos(aux_is_x) * dret\n",
        "else:\n",
        "    # sin saves cos(x) in Aux - may save compute if `sincos`` is faster than `sin` and `cos`\n",
        "    def sin_fwd(x):\n",
        "        ret, aux = torch.sincos(x)\n",
        "        return ret, aux\n",
        "    def sin_bwd(aux_is_cosx,dret): \n",
        "        return aux_is_cosx * dret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chain rule\n",
        "\n",
        "An FX traced function is of the general form\n",
        "```py\n",
        "def foo(a1..an):\n",
        "  t1 = f1(a1..an)\n",
        "  t2 =  f2(a1..an,t1) # wlog, fk uses all in-scope variables\n",
        "  ...\n",
        "  tm =     fm(a1..an,t1..t{m-1}) \n",
        "  return tm\n",
        "```\n",
        "Then the VJP (vector-jacobian product) is of the form\n",
        "```py\n",
        "def foo_vjp(a1..an, dtm):\n",
        "  # Forward pass\n",
        "  t1, aux1               = f1_fwd(a1..an)\n",
        "  t2, aux2               =   f2_fwd(a1..an,t1)\n",
        "  ...\n",
        "  tm, auxm               =      fm_fwd(a1..an,t1..t{m-1})\n",
        "\n",
        "  # Backward pass\n",
        "  da1..dan,dt1..dt{m-1} +=      fm_bwd(auxm, dtm)\n",
        "  ...\n",
        "  da{1..n},dt1          +=   f2_bwd(aux2, dt3)\n",
        "  da{1..n}              += f1_bwd(aux1, dt1)\n",
        "\n",
        "  return da{1..n}\n",
        "```\n",
        "\n",
        "So let's make a transformer `fx_vjp` that does that.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# A mapping from python function to (forward, backward)\n",
        "ad_map = {}\n",
        "ad_map[torch.sin] = sin_fwd, sin_bwd\n",
        "\n",
        "def fx_vjp(f, sample_input):\n",
        "    \"\"\"\n",
        "    An FX transform that implements reverse-mode automatic differentiation.\n",
        "\n",
        "    If the traced function is of the form\n",
        "    ```py\n",
        "    def foo(a1..an):\n",
        "      t1 = f1(a1..an)\n",
        "      t2 =  f2(a1..an,t1) # wlog, fk uses all in-scope variables\n",
        "      ...\n",
        "      tm =   fm(a1..an,t1..t{m-1}) \n",
        "      return tm\n",
        "    ```\n",
        "    Then the VJP (vector-jacobian product is of the form)\n",
        "    ```py\n",
        "    def foo_vjp(a1..an, dtm):\n",
        "      t1, aux1 = f1_fwd(a1..an)\n",
        "      t2, aux2 = f2_fwd(a1..an,t1)\n",
        "      ...\n",
        "      tm, auxm = fm_fwd(a1..an,t1..{m-1})\n",
        "\n",
        "      da{1..n},dt{1..m-1} += fm_bwd(auxm, dtm)\n",
        "      ...\n",
        "      da{1..n},dt1 += f2_bwd(aux2, dt3)\n",
        "      da{1..n} += f1_bwd(aux1, dt1)\n",
        "\n",
        "      return da{1..n}\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    class ADInterpreter(AnnotatingInterpreter):\n",
        "        \"\"\"\n",
        "        This interpreter runs through the forward transformation, \n",
        "        replacing calls to `fk` with `fk_fwd = ad_map[fk][0]`,\n",
        "        and recording the operations on a stack.\n",
        "        \"\"\"\n",
        "        def __init__(self, f):\n",
        "            super().__init__(f)\n",
        "            self.stack = []\n",
        "\n",
        "        def call_function(self, target, args, kwargs):\n",
        "            assert kwargs == None or len(kwargs) == 0\n",
        "\n",
        "            if target not in ad_map:\n",
        "                raise NotImplementedError(f\"Need VJP rule for {target}\")\n",
        "            # Look up forward/backward functions in `ad_map`\n",
        "            fwd,bwd = ad_map[target]\n",
        "            # Call the fwd function, getting proxies for returns\n",
        "            val,aux = fwd(*args)\n",
        "            # In the backward pass, we will compute:\n",
        "            #  d[args[0]],...,d[args[-1]] = bwd(aux, d{val})\n",
        "            # So remember: (args, bwd, aux, val)\n",
        "            # Note that all of these are symbolic, so it's cheap to remember them\n",
        "            self.stack.append((args, bwd, aux, val))\n",
        "            # And return the return value (a proxy)\n",
        "            return val\n",
        "\n",
        "        def call_method(self, target, args, kwargs):\n",
        "            raise NotImplementedError # use method_to_function\n",
        "\n",
        "        def get_attr(self, target, args, kwargs):\n",
        "            raise NotImplementedError # TODO\n",
        "\n",
        "    # Grab the FX graph\n",
        "    f_trace = torch.fx.symbolic_trace(f)\n",
        "    \n",
        "    # Run shape analysis, record answers in the graph\n",
        "    fx_add_shapes(f_trace, sample_input)\n",
        "\n",
        "    # This is the \"template\" function for the VJP\n",
        "    def vjp_template(x, dret):\n",
        "        # Run the forward computations, and collect them in ad.stack\n",
        "        ad = ADInterpreter(f_trace)\n",
        "        ret = ad.run(x)\n",
        "        # Build a dict to hold derivatives\n",
        "        d =  defaultdict(lambda: 0)\n",
        "        # Add dret to derivatives dict\n",
        "        d[ret] = dret\n",
        "        # And run down the stack...\n",
        "        for (args, bwd, aux, val) in reversed(ad.stack):\n",
        "            dargs = bwd(aux, d[val])\n",
        "            for (a,da) in zip(args, ensure_tuple(dargs)):\n",
        "                d[a] += da\n",
        "        # And return ret and J'*dret\n",
        "        return ret, d[x]\n",
        "\n",
        "    # Trace through vjp_template and return.\n",
        "    return torch.fx.symbolic_trace(vjp_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## And now define the AD rules\n",
        "\n",
        "These are \"just python\", nothing is built in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import vjp_rules\n",
        "from vjp_check import vjp_check\n",
        "\n",
        "def vjp_linear(f):\n",
        "    \"\"\"\n",
        "    Construct fwd and bwd for f a linear function of x\n",
        "    \"\"\"\n",
        "    def fwd(*args): return f(*args), None\n",
        "    def bwd(_, dret): return f(dret)\n",
        "    return fwd, bwd\n",
        "\n",
        "\n",
        "ad_map_aux = {\n",
        "    operator.neg: vjp_linear(operator.neg),\n",
        "    operator.add: (vjp_rules.add_fwd, vjp_rules.add_bwd),\n",
        "    operator.mul: (vjp_rules.mul_fwd, vjp_rules.mul_bwd),\n",
        "    operator.matmul: (vjp_rules.matmul_fwd, vjp_rules.matmul_bwd),\n",
        "    torch.neg: vjp_linear(torch.neg),\n",
        "    torch.sum: vjp_linear(torch.sum),\n",
        "    torch.relu: (vjp_rules.relu_fwd, vjp_rules.relu_bwd),\n",
        "    torch.transpose: (lambda x,m,n: (torch.transpose(x,m,n), (m,n)),\n",
        "                      lambda aux,dret: torch.transpose(dret,*aux)),\n",
        "    torch.diag: (vjp_rules.diag_fwd, vjp_rules.diag_bwd),\n",
        "    vjp_rules.scale: (vjp_rules.scale_fwd, vjp_rules.scale_bwd),\n",
        "}\n",
        "\n",
        "ad_map = {**ad_map, **ad_map_aux}\n",
        "\n",
        "def fx_trace_fwd(x):\n",
        "    assert len(fx_shape(x)) == 2\n",
        "    return torch.trace(x), fx_shape(x)\n",
        "\n",
        "# Register custom passes for trace (as we needed to use fx_shape)\n",
        "ad_map[torch.trace] = (fx_trace_fwd, vjp_rules.trace_bwd)\n",
        "\n",
        "# And let's add some shape checking to add and mul, as we have it...\n",
        "def fx_add_fwd(A,B):\n",
        "    assert fx_shape(A) == fx_shape(B)\n",
        "    return vjp_rules.add_fwd(A,B)\n",
        "ad_map[operator.add] = (fx_add_fwd, vjp_rules.add_bwd)\n",
        "\n",
        "def fx_mul_fwd(A,B):\n",
        "    assert fx_shape(A) == fx_shape(B)\n",
        "    return vjp_rules.mul_fwd(A,B)\n",
        "ad_map[operator.mul] = (fx_mul_fwd, vjp_rules.mul_bwd)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VJPs match OK\n",
            "\n",
            "torch.fx._symbolic_trace.wrap(\"vjp_rules_scale\")\n",
            "\n",
            "def forward(self, x, dret):\n",
            "    trace = torch.trace(x)\n",
            "    sin = torch.sin(trace)\n",
            "    scale = vjp_rules_scale(sin, x)\n",
            "    mul = x * dret;  x = None\n",
            "    sum_1 = torch.sum(mul);  mul = None\n",
            "    scale_1 = vjp_rules_scale(sin, dret);  sin = dret = None\n",
            "    add = 0 + sum_1;  sum_1 = None\n",
            "    add_1 = 0 + scale_1;  scale_1 = None\n",
            "    cos = torch.cos(trace);  trace = None\n",
            "    mul_1 = cos * add;  cos = add = None\n",
            "    add_2 = 0 + mul_1;  mul_1 = None\n",
            "    _tensor_constant0 = self._tensor_constant0\n",
            "    mul_2 = add_2 * _tensor_constant0;  add_2 = _tensor_constant0 = None\n",
            "    add_3 = add_1 + mul_2;  add_1 = mul_2 = None\n",
            "    return (scale, add_3)\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "# Function to vjp\n",
        "def foo(x):\n",
        "    w = torch.trace(x)\n",
        "    w = torch.sin(w)\n",
        "    a = vjp_rules.scale(w, x)\n",
        "    return a\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "x = torch.randn(3,3)\n",
        "foo_vjp = fx_vjp(foo, x)\n",
        "\n",
        "dret = torch.randn_like(foo(x))\n",
        "foo_vjp_pt = lambda x,dret: torch.autograd.functional.vjp(foo, x, dret)\n",
        "\n",
        "PyTree.assert_close(foo_vjp_pt(x,dret), foo_vjp(x, dret))\n",
        "print('VJPs match OK')\n",
        "\n",
        "print(foo_vjp.code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VJPs match\n"
          ]
        }
      ],
      "source": [
        "# Manual VJP to compare to\n",
        "def foo_vjp_manual(x, dret):\n",
        "    w = torch.trace(x)\n",
        "    w1 = torch.sin(w)\n",
        "    ret = w1 * x\n",
        "    \n",
        "    dw1 = torch.sum(dret * x)\n",
        "    dx = w1 * dret\n",
        "\n",
        "    dw = torch.cos(w) * dw1\n",
        "\n",
        "    dx += dw * torch.eye(*x.shape)\n",
        "    \n",
        "    return ret, dx\n",
        "\n",
        "PyTree.assert_close(foo_vjp_manual(x, dret), foo_vjp_pt(x,dret))\n",
        "print('VJPs match')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---------------\n",
        "Misc below here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ic| bar_vjp_manual(x, dret): (tensor([[ 8.3484, -3.1498],\n",
            "                                     [-3.1498,  3.7344]]),\n",
            "                              tensor([[ 0.5019, -0.6084],\n",
            "                                     [-1.3575,  1.0871],\n",
            "                                     [-1.5760,  1.8188],\n",
            "                                     [-4.3323, -0.3791],\n",
            "                                     [ 2.5767, -1.5431]]))\n",
            "ic| bar_vjp(x, dret): (tensor([[ 2.4017, -0.9061],\n",
            "                              [-0.9061,  1.0743]]),\n",
            "                       tensor([[ 0.0694, -0.1907],\n",
            "                              [-0.2250,  0.3101],\n",
            "                              [-0.2241,  0.5651],\n",
            "                              [-0.9746, -0.4284],\n",
            "                              [ 0.4618, -0.3968]]))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VJP OK: <function bar at 0x7f085d69ff40>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Function to vjp\n",
        "def bar(x):\n",
        "    t = torch.transpose(x, 1, 0)\n",
        "    v = operator.matmul(t, x)\n",
        "\n",
        "    # w = torch.trace(v)\n",
        "    # ret = vjp_rules.scale(w, v)\n",
        "    \n",
        "    return v\n",
        "\n",
        "def bar_vjp_manual(x, dret):\n",
        "    x1, x2 = x,x\n",
        "    t = torch.transpose(x1, 1, 0)\n",
        "    v,aux = vjp_rules.matmul_fwd(t, x2)\n",
        "\n",
        "    v1,v2 = v,v\n",
        "    w = torch.trace(v1)\n",
        "    ret = w * v2\n",
        "    \n",
        "    dw = (dret * v2).sum()\n",
        "    dv2 = w * dret\n",
        "    dv1 = dw * torch.eye(*v1.shape)\n",
        "    \n",
        "    dv = dv1 + dv2\n",
        "    dt,dx2 = vjp_rules.matmul_bwd(aux, dv)\n",
        "\n",
        "    dx1 = torch.transpose(dt, 1, 0)\n",
        "    dx = dx1 + dx2\n",
        "    return ret, dx\n",
        "dret = torch.randn_like(foo(x))\n",
        "\n",
        "x = torch.randn(5,2)\n",
        "\n",
        "bar_vjp = fx_vjp(bar, x)\n",
        "\n",
        "dret = torch.randn_like(bar(x))\n",
        "ic(bar_vjp_manual(x, dret))\n",
        "ic(bar_vjp(x, dret))\n",
        "\n",
        "vjp_check(bar, bar_vjp, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "class vjp_template(torch.nn.Module):\n",
            "    torch.fx._symbolic_trace.wrap(\"vjp_rules_scale\")\n",
            "    \n",
            "    def forward(self, x, dret):\n",
            "        \n",
            "        # No stacktrace found for following nodes \n",
            "        trace = torch.trace(x)\n",
            "        sin = torch.sin(trace)\n",
            "        scale = vjp_rules_scale(sin, x)\n",
            "        mul = x * dret;  x = None\n",
            "        sum_1 = torch.sum(mul);  mul = None\n",
            "        scale_1 = vjp_rules_scale(sin, dret);  sin = dret = None\n",
            "        add = 0 + sum_1;  sum_1 = None\n",
            "        add_1 = 0 + scale_1;  scale_1 = None\n",
            "        cos = torch.cos(trace);  trace = None\n",
            "        mul_1 = cos * add;  cos = add = None\n",
            "        add_2 = 0 + mul_1;  mul_1 = None\n",
            "        _tensor_constant0 = self._tensor_constant0\n",
            "        mul_2 = add_2 * _tensor_constant0;  add_2 = _tensor_constant0 = None\n",
            "        add_3 = add_1 + mul_2;  add_1 = mul_2 = None\n",
            "        return (scale, add_3)\n",
            "        \n"
          ]
        }
      ],
      "source": [
        "foo_vjp.print_readable()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using FX IR to print source code of functorch.jacrev(f)\n",
        "\n",
        "This is rather more low-level than the AD above, as it reflects the operations that hit the torch dispatcher.\n",
        "This also means it is size-specialized. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "def forward(self, x_1):\n",
            "    sin = torch.ops.aten.sin.default(x_1)\n",
            "    _tensor_constant0 = self._tensor_constant0\n",
            "    lift_fresh_copy = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None\n",
            "    cumsum = torch.ops.aten.cumsum.default(lift_fresh_copy, 0);  lift_fresh_copy = None\n",
            "    slice_1 = torch.ops.aten.slice.Tensor(cumsum, 0, 0, -1);  cumsum = None\n",
            "    neg = torch.ops.aten.neg.default(slice_1);  slice_1 = None\n",
            "    unbind = torch.ops.aten.unbind.int(neg);  neg = None\n",
            "    new_zeros = torch.ops.aten.new_zeros.default(sin, [13, 13], dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False);  sin = None\n",
            "    diagonal = torch.ops.aten.diagonal.default(new_zeros)\n",
            "    fill_ = torch.ops.aten.fill_.Scalar(diagonal, 1);  diagonal = None\n",
            "    view = torch.ops.aten.view.default(new_zeros, [13, 13]);  new_zeros = None\n",
            "    cos = torch.ops.aten.cos.default(x_1);  x_1 = None\n",
            "    mul = torch.ops.aten.mul.Tensor(view, cos);  view = cos = None\n",
            "    split_with_sizes = torch.ops.aten.split_with_sizes.default(mul, [13]);  mul = None\n",
            "    getitem = split_with_sizes[0];  split_with_sizes = None\n",
            "    view_1 = torch.ops.aten.view.default(getitem, [13, 13]);  getitem = None\n",
            "    return view_1\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "from functorch import make_fx, grad, vjp, jacrev\n",
        "def f(x):\n",
        "    return torch.sin(x)\n",
        "x = torch.randn(13)\n",
        "grad_f = make_fx(jacrev(f))(x)\n",
        "print(grad_f.code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For comparison, the FX AD version is closer to what one might write by hand:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "def forward(self, x, dret):\n",
            "    sin = torch.sin(x)\n",
            "    cos = torch.cos(x);  x = None\n",
            "    mul = cos * dret;  cos = dret = None\n",
            "    add = 0 + mul;  mul = None\n",
            "    return (sin, add)\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "print(fx_vjp(f, x).code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Trying to get functorch vjp working.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "only integer tensors of a single element can be converted to an index",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/awf/dev/gh-awf/fx-tools/DiffFX_Experiments.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/awf/dev/gh-awf/fx-tools/DiffFX_Experiments.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m10\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/awf/dev/gh-awf/fx-tools/DiffFX_Experiments.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m f_vjp \u001b[39m=\u001b[39m functorch\u001b[39m.\u001b[39mvjp(f,x)[\u001b[39m1\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/awf/dev/gh-awf/fx-tools/DiffFX_Experiments.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m grad_f \u001b[39m=\u001b[39m make_fx(f_vjp)(x, x, \u001b[39m1\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/awf/dev/gh-awf/fx-tools/DiffFX_Experiments.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(grad_f(x))\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/fx/experimental/proxy_tensor.py:652\u001b[0m, in \u001b[0;36mmake_fx.<locals>.wrapped\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[39m# We disable the autocast cache as the autocast cache causes type conversions on parameters to\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[39m# check a cache, which introduces untracked tensors into the graph\u001b[39;00m\n\u001b[1;32m    650\u001b[0m \u001b[39mwith\u001b[39;00m decompose(decomposition_table), fake_tensor_mode, python_dispatcher_mode, \\\n\u001b[1;32m    651\u001b[0m      sym_mode, proxy_mode, disable_autocast_cache():  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m     t \u001b[39m=\u001b[39m dispatch_trace(wrap_key(func, args, fx_tracer), tracer\u001b[39m=\u001b[39;49mfx_tracer, concrete_args\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(phs))\n\u001b[1;32m    654\u001b[0m \u001b[39m# TODO: kind of a bad way to do it, should maybe figure out a better way\u001b[39;00m\n\u001b[1;32m    655\u001b[0m \u001b[39mif\u001b[39;00m tracing_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msymbolic\u001b[39m\u001b[39m\"\u001b[39m:\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/fx/experimental/proxy_tensor.py:400\u001b[0m, in \u001b[0;36mdispatch_trace\u001b[0;34m(root, tracer, concrete_args)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdispatch_trace\u001b[39m(\n\u001b[1;32m    396\u001b[0m         root: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, Callable],\n\u001b[1;32m    397\u001b[0m         tracer: Tracer,\n\u001b[1;32m    398\u001b[0m         concrete_args: Optional[Tuple[Any, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    399\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m GraphModule:\n\u001b[0;32m--> 400\u001b[0m     graph \u001b[39m=\u001b[39m tracer\u001b[39m.\u001b[39;49mtrace(root, concrete_args)\n\u001b[1;32m    401\u001b[0m     name \u001b[39m=\u001b[39m root\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(root, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule) \u001b[39melse\u001b[39;00m root\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m    402\u001b[0m     \u001b[39mreturn\u001b[39;00m GraphModule(tracer\u001b[39m.\u001b[39mroot, graph, name)\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/fx/_symbolic_trace.py:739\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_autowrap_search:\n\u001b[1;32m    733\u001b[0m             _autowrap_check(\n\u001b[1;32m    734\u001b[0m                 patcher, module\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_autowrap_function_ids\n\u001b[1;32m    735\u001b[0m             )\n\u001b[1;32m    736\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_node(\n\u001b[1;32m    737\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    738\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m--> 739\u001b[0m             (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_arg(fn(\u001b[39m*\u001b[39;49margs)),),\n\u001b[1;32m    740\u001b[0m             {},\n\u001b[1;32m    741\u001b[0m             type_expr\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__annotations__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mreturn\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    742\u001b[0m         )\n\u001b[1;32m    744\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubmodule_paths \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/fx/experimental/proxy_tensor.py:414\u001b[0m, in \u001b[0;36mwrap_key.<locals>.wrapped\u001b[0;34m(*proxies)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(flat_proxies) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(flat_tensors)\n\u001b[1;32m    412\u001b[0m track_tensor_tree(flat_tensors, flat_proxies, constant\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, tracer\u001b[39m=\u001b[39mtracer)\n\u001b[0;32m--> 414\u001b[0m out \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49mtensors)\n\u001b[1;32m    415\u001b[0m out \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_map_only(\n\u001b[1;32m    416\u001b[0m     torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    417\u001b[0m     \u001b[39mlambda\u001b[39;00m t: get_proxy_slot(t, tracer, t, \u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mproxy),\n\u001b[1;32m    418\u001b[0m     out\n\u001b[1;32m    419\u001b[0m )\n\u001b[1;32m    420\u001b[0m out \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_map_only(\n\u001b[1;32m    421\u001b[0m     (SymInt, SymFloat),\n\u001b[1;32m    422\u001b[0m     \u001b[39mlambda\u001b[39;00m t: get_proxy_slot(t\u001b[39m.\u001b[39mget_pyobj(), tracer)(),\n\u001b[1;32m    423\u001b[0m     out\n\u001b[1;32m    424\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/functorch/_src/eager_transforms.py:323\u001b[0m, in \u001b[0;36m_vjp_with_argnums.<locals>.wrapper\u001b[0;34m(cotangents, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m primals_out_spec \u001b[39m!=\u001b[39m cotangents_spec:\n\u001b[1;32m    318\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mExpected pytree structure of cotangents to be the same \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mas pytree structure of outputs to the function. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcotangents: \u001b[39m\u001b[39m{\u001b[39;00mtreespec_pprint(cotangents_spec)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    322\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mprimal output: \u001b[39m\u001b[39m{\u001b[39;00mtreespec_pprint(primals_out_spec)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 323\u001b[0m result \u001b[39m=\u001b[39m _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n\u001b[1;32m    324\u001b[0m                         retain_graph\u001b[39m=\u001b[39;49mretain_graph, create_graph\u001b[39m=\u001b[39;49mcreate_graph)\n\u001b[1;32m    325\u001b[0m \u001b[39mreturn\u001b[39;00m tree_unflatten(result, primals_spec)\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/functorch/_src/eager_transforms.py:113\u001b[0m, in \u001b[0;36m_autograd_grad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(diff_outputs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    112\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(torch\u001b[39m.\u001b[39mzeros_like(inp) \u001b[39mfor\u001b[39;00m inp \u001b[39min\u001b[39;00m inputs)\n\u001b[0;32m--> 113\u001b[0m grad_inputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(diff_outputs, inputs, grad_outputs,\n\u001b[1;32m    114\u001b[0m                                   retain_graph\u001b[39m=\u001b[39;49mretain_graph,\n\u001b[1;32m    115\u001b[0m                                   create_graph\u001b[39m=\u001b[39;49mcreate_graph,\n\u001b[1;32m    116\u001b[0m                                   allow_unused\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    117\u001b[0m grad_inputs \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(torch\u001b[39m.\u001b[39mzeros_like(inp) \u001b[39mif\u001b[39;00m gi \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m gi\n\u001b[1;32m    118\u001b[0m                     \u001b[39mfor\u001b[39;00m gi, inp \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(grad_inputs, inputs))\n\u001b[1;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m grad_inputs\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/autograd/__init__.py:300\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    299\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 300\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    301\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    302\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
            "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
          ]
        }
      ],
      "source": [
        "import functorch\n",
        "from functorch import make_fx, grad, vjp, jacrev\n",
        "def f(x):\n",
        "    return torch.sin(x)\n",
        "x = torch.randn(10)\n",
        "f_vjp = functorch.vjp(f,x)[1]\n",
        "grad_f = make_fx(f_vjp)(x, x, 1)\n",
        "print(grad_f(x))\n",
        "#print(torch.fx.symbolic_trace(grad_f)(x).code)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMk6IojwKPOoEtWLs82Civ+",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 ('pytorch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "28f0c4cf9ff11e7fed1023153201d84a0ac9765962dc65ba2242229f995562f8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
