{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DiffFX Derivation\n",
        "\n",
        "This notebook is a walkthrough of the initial design and implementation of difffx,\n",
        "a simple source-to-source autodiff tool using Torch FX.\n",
        "See https://github.com/pytorch/pytorch/blob/master/torch/fx/OVERVIEW.md#the-fx-ir for\n",
        "an overview of the FX IR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Import prerequisites, define utility functions\n",
        "from collections import defaultdict\n",
        "\n",
        "import operator\n",
        "\n",
        "import torch\n",
        "import torch.fx as fx\n",
        "\n",
        "from icecream import ic \n",
        "\n",
        "from awfutils.pytree_utils import PyTree\n",
        "\n",
        "from vjp_check import vjp_check\n",
        "\n",
        "def ensure_tuple(x):\n",
        "    if isinstance(x, tuple):\n",
        "        return x\n",
        "    return (x,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FX_VJP: Source-to-source reverse mode algorithmic differentiation via FX \n",
        "\n",
        "An FX Interpreter that implements reverse-mode automatic differentiation.\n",
        "\n",
        "It may help to recall the basic laws of AD.\n",
        "Given function `f` which takes arbitrary pytree type `S`, and returns arbitary pytree `T`,\n",
        "we define the vector-Jacobian product `vjp{f}` which takes types `(S, dT)` and returns type `dS` with `vjp{f}(s, dt) = dt * J{f}(s)`.\n",
        "When `T` is a scalar or has only one element, then the VJP is the gradient,\n",
        "so `grad{f}(s) = vjp{f}(s, 1.0)`.\n",
        "```py\n",
        "def f(s : S) -> T: \n",
        "  ...\n",
        "def vjp{f}(s : S, dt : dT) -> dS: \n",
        "  ...\n",
        "```\n",
        "and as we generally divide the computation into\n",
        "'forward' and 'backward' passes, returning the result of `f` in the forward pass,\n",
        "as well as some auxiliary information of type `Aux{f}` \n",
        "```py\n",
        "def fwd{f}(s : S) -> (T, Aux{f}): \n",
        "  ...\n",
        "def bwd{f}(aux : Aux{f}, dt : dT) -> dS:\n",
        "  ...\n",
        "```\n",
        "in terms of which we could just write `vjp{f}` as\n",
        "```py\n",
        "def vjp{f}(s : S, dt : dT) -> dS:\n",
        "  _t, Aux = fwd{f}(s)\n",
        "  return bwd{f}(Aux, dt)\n",
        "```\n",
        "\n",
        "Here's an example for `sin`, where we provide two implementations, \n",
        "one likely to be more memory efficient, the other likely to be faster. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: sin\n",
        "if True:\n",
        "    # sin saves x in Aux - may save memory as x is likely preserved for backward pass\n",
        "    def sin_fwd(x):\n",
        "        return (torch.sin(x), x)\n",
        "    def sin_bwd(aux_is_x,dret): \n",
        "        return torch.cos(aux_is_x) * dret\n",
        "else:\n",
        "    # sin saves cos(x) in Aux - may save compute if `sincos`` is faster than `sin` and `cos`\n",
        "    def sin_fwd(x):\n",
        "        ret, aux = torch.sincos(x)\n",
        "        return ret, aux\n",
        "    def sin_bwd(aux_is_cosx,dret): \n",
        "        return aux_is_cosx * dret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chain rule\n",
        "\n",
        "An FX traced function is of the general form\n",
        "```py\n",
        "def foo(a1..an):\n",
        "  t1 = f1(a1..an)\n",
        "  t2 =  f2(a1..an,t1) # wlog, fk uses all in-scope variables\n",
        "  ...\n",
        "  tm =     fm(a1..an,t1..t{m-1}) \n",
        "  return tm\n",
        "```\n",
        "Then the VJP (vector-jacobian product) is of the form\n",
        "```py\n",
        "def foo_vjp(a1..an, dtm):\n",
        "  # Forward pass\n",
        "  t1, aux1               = f1_fwd(a1..an)\n",
        "  t2, aux2               =   f2_fwd(a1..an,t1)\n",
        "  ...\n",
        "  tm, auxm               =      fm_fwd(a1..an,t1..t{m-1})\n",
        "\n",
        "  # Backward pass\n",
        "  da1..dan,dt1..dt{m-1} +=      fm_bwd(auxm, dtm)\n",
        "  ...\n",
        "  da{1..n},dt1          +=   f2_bwd(aux2, dt3)\n",
        "  da{1..n}              += f1_bwd(aux1, dt1)\n",
        "\n",
        "  return da{1..n}\n",
        "```\n",
        "\n",
        "So let's make a transformer `fx_vjp` that does that.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# A mapping from python function to (forward, backward)\n",
        "ad_map = {}\n",
        "\n",
        "# Register the sin functions as above\n",
        "ad_map[torch.sin] = sin_fwd, sin_bwd\n",
        "\n",
        "# Get shnty tracers\n",
        "from fx_shnty import shnty_trace, abstractify, get_return_abstract_value\n",
        "\n",
        "def fx_vjp(f, sample_input):\n",
        "    \"\"\"\n",
        "    An FX transform that implements reverse-mode automatic differentiation.\n",
        "\n",
        "    If the traced function is of the form\n",
        "    ```py\n",
        "    def foo(a1..an):\n",
        "      t1 = f1(a1..an)\n",
        "      t2 =  f2(a1..an,t1) # wlog, fk uses all in-scope variables\n",
        "      ...\n",
        "      tm =   fm(a1..an,t1..t{m-1}) \n",
        "      return tm\n",
        "    ```\n",
        "    Then the VJP (vector-jacobian product is of the form)\n",
        "    ```py\n",
        "    def foo_vjp(a1..an, dtm):\n",
        "      t1, aux1 = f1_fwd(a1..an)\n",
        "      t2, aux2 = f2_fwd(a1..an,t1)\n",
        "      ...\n",
        "      tm, auxm = fm_fwd(a1..an,t1..{m-1})\n",
        "\n",
        "      da{1..n},dt{1..m-1} += fm_bwd(auxm, dtm)\n",
        "      ...\n",
        "      da{1..n},dt1 += f2_bwd(aux2, dt3)\n",
        "      da{1..n} += f1_bwd(aux1, dt1)\n",
        "\n",
        "      return da{1..n}\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    class ADInterpreter(torch.fx.Interpreter):\n",
        "        \"\"\"\n",
        "        This interpreter runs through the forward transformation, \n",
        "        replacing calls to `fk` with `fk_fwd = ad_map[fk][0]`,\n",
        "        and recording the operations on a stack.\n",
        "        \"\"\"\n",
        "        def __init__(self, f):\n",
        "            super().__init__(f)\n",
        "            self.stack = []\n",
        "\n",
        "        def call_function(self, target, args, kwargs):\n",
        "            assert kwargs == None or len(kwargs) == 0\n",
        "\n",
        "            if target not in ad_map:\n",
        "                raise NotImplementedError(f\"Need VJP rule for {target}\")\n",
        "            # Look up forward/backward functions in `ad_map`\n",
        "            fwd,bwd = ad_map[target]\n",
        "            # Call the fwd function, getting proxies for returns\n",
        "            val,aux = fwd(*args)\n",
        "            # In the backward pass, we will compute:\n",
        "            #  d[args[0]],...,d[args[-1]] = bwd(aux, d{val})\n",
        "            # So remember: (args, bwd, aux, val)\n",
        "            # Note that all of these are symbolic, so it's cheap to remember them\n",
        "            self.stack.append((args, bwd, aux, val))\n",
        "            # And return the return value (a proxy)\n",
        "            return val\n",
        "\n",
        "        def call_method(self, target, args, kwargs):\n",
        "            raise NotImplementedError # use method_to_function\n",
        "\n",
        "        def get_attr(self, target, args, kwargs):\n",
        "            raise NotImplementedError # TODO\n",
        "\n",
        "    # Grab the FX graph, using shnty_trace to get shapes\n",
        "    f_trace = shnty_trace(f, sample_input)\n",
        "    \n",
        "    # This is the \"template\" function for the VJP\n",
        "    def vjp_template(x, dret):\n",
        "        # Run the forward computations, and collect them in ad.stack\n",
        "        ad = ADInterpreter(f_trace)\n",
        "        ret = ad.run(x)\n",
        "        # Build a dict to hold derivatives\n",
        "        d =  defaultdict(lambda: 0)\n",
        "        # Add dret to derivatives dict\n",
        "        d[ret] = dret\n",
        "        # And run down the stack...\n",
        "        for (args, bwd, aux, val) in reversed(ad.stack):\n",
        "            dargs = bwd(aux, d[val])\n",
        "            for (a,da) in zip(args, ensure_tuple(dargs)):\n",
        "                d[a] += da\n",
        "        # And return ret and J'*dret\n",
        "        return ret, d[x]\n",
        "\n",
        "    # Trace through vjp_template and return.\n",
        "    return shnty_trace(vjp_template, sample_input + (get_return_abstract_value(f_trace),))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## And now define the AD rules\n",
        "\n",
        "These are \"just python\", nothing is built in. We define VJPs for just a couple of primitives here, and with minimal error checking.\n",
        "See `vjp_rules.py` for more primitives (e.g. `mul` rather than the special case of `scale` here.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Trace\n",
        "def trace_fwd(x):\n",
        "    return torch.trace(x), x.shape\n",
        "\n",
        "\n",
        "def trace_bwd(x_shape, dret):\n",
        "    return dret * torch.eye(*x_shape)\n",
        "\n",
        "ad_map[torch.trace] = (trace_fwd, trace_bwd)\n",
        "\n",
        "\n",
        "# Add\n",
        "def add_fwd(A, B):\n",
        "    assert A.shape == B.shape\n",
        "    return A + B, None\n",
        "\n",
        "\n",
        "def add_bwd(aux, dret):\n",
        "    return dret, dret\n",
        "\n",
        "ad_map[operator.add] = (add_fwd, add_bwd)\n",
        "\n",
        "# Special case of mul (scalar * Tensor)\n",
        "def mul_fwd(a, T):\n",
        "    return a * T, (a, T)\n",
        "\n",
        "def mul_bwd(aux, dret):\n",
        "    # T: mxn\n",
        "    # dret: mxn\n",
        "    a, T = aux\n",
        "    da = torch.sum(T * dret)\n",
        "    dT = a * dret\n",
        "    return da, dT\n",
        "\n",
        "ad_map[operator.mul] = (mul_fwd, mul_bwd)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ... and let's try it out\n",
        "\n",
        "We define a function `foo`, compute its vjp using DiffFX, and compare to `autograd.functional.vjp`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VJP matches PT\n"
          ]
        }
      ],
      "source": [
        "# Function to vjp\n",
        "def foo(x):\n",
        "    h = torch.trace(x)\n",
        "    w = torch.sin(h)\n",
        "    y = w * x\n",
        "    z = w * torch.sin(x)\n",
        "    return y + z\n",
        "\n",
        "import fx_shnty\n",
        "fx_shnty._log = lambda x:...\n",
        "torch.manual_seed(42)\n",
        "\n",
        "x = torch.randn(3,3)\n",
        "foo_vjp = fx_vjp(foo, (abstractify(x),))\n",
        "\n",
        "dret = torch.randn_like(foo(x))\n",
        "foo_vjp_pt = lambda x,dret: torch.autograd.functional.vjp(foo, x, dret)\n",
        "\n",
        "PyTree.assert_close(foo_vjp(x,dret), foo_vjp_pt(x, dret))\n",
        "print('VJP matches PT')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now let's show that it's a real source-to-source transformation, by printing the code for the VJP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def vjp_template(x,dret):\n",
            "  v10 = x\n",
            "  v11 = dret\n",
            "  v12 = torch.trace(v10)\n",
            "  v13 = torch.sin(v12)\n",
            "  v14 = mul(v13,v10)\n",
            "  v15 = torch.sin(v10)\n",
            "  v16 = mul(v13,v15)\n",
            "  v17 = add(v14,v16)\n",
            "  v18 = add(int(0),v11)\n",
            "  v19 = add(int(0),v11)\n",
            "  v20 = mul(v15,v19)\n",
            "  v21 = torch.sum(v20)\n",
            "  v22 = mul(v13,v19)\n",
            "  v23 = add(int(0),v21)\n",
            "  v24 = add(int(0),v22)\n",
            "  v25 = torch.cos(v10)\n",
            "  v26 = mul(v25,v24)\n",
            "  v27 = add(int(0),v26)\n",
            "  v28 = mul(v10,v18)\n",
            "  v29 = torch.sum(v28)\n",
            "  v30 = mul(v13,v18)\n",
            "  v31 = add(v23,v29)\n",
            "  v32 = add(v27,v30)\n",
            "  v33 = torch.cos(v12)\n",
            "  v34 = mul(v33,v31)\n",
            "  v35 = add(int(0),v34)\n",
            "  v36 = _tensor_constant0 # tensor([[1., 0., 0.],\\n        [0., 1., \n",
            "  v37 = mul(v35,v36)\n",
            "  v38 = add(v32,v37)\n",
            "  return (v17,v38)\n"
          ]
        }
      ],
      "source": [
        "from fx_print import fx_print\n",
        "fx_print(foo_vjp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using FX IR to print source code of functorch.jacrev(f)\n",
        "\n",
        "This is rather more low-level than the AD above, as it reflects the operations that hit the torch dispatcher.\n",
        "Let's first print the DiffFX gradient for a simple function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor(-0.8336), tensor([[0.2229, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.2229, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.2229, 0.0000, 0.0000]]))\n",
            "def vjp_template(x,dret):\n",
            "  v10 = x\n",
            "  v11 = dret\n",
            "  v12 = add(v10,v10)\n",
            "  v13 = torch.trace(v12)\n",
            "  v14 = torch.sin(v13)\n",
            "  v15 = torch.cos(v13)\n",
            "  v16 = mul(v15,v11)\n",
            "  v17 = add(int(0),v16)\n",
            "  v18 = _tensor_constant0 # tensor([[1., 0., 0., 0., 0.],\\n        [\n",
            "  v19 = mul(v17,v18)\n",
            "  v20 = add(int(0),v19)\n",
            "  v21 = add(int(0),v20)\n",
            "  v22 = add(v21,v20)\n",
            "  return (v14,v22)\n"
          ]
        }
      ],
      "source": [
        "def f(x):\n",
        "    z = x + x\n",
        "    y = torch.trace(z)\n",
        "    return torch.sin(y)\n",
        "\n",
        "x = torch.randn(3,5)\n",
        "f_vjp = fx_vjp(f, (abstractify(x),))\n",
        "\n",
        "dret = torch.rand_like(f(x))\n",
        "print(f_vjp(x, dret))\n",
        "\n",
        "fx_print(f_vjp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And then the gradient from jacrev:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def f(x_1):\n",
            "  v10 = x_1\n",
            "  v11 = torch._ops.aten.add.Tensor(v10,v10)\n",
            "  v12 = torch._ops.aten.trace.default(v11)\n",
            "  v13 = torch._ops.aten.sin.default(v12)\n",
            "  v14 = _tensor_constant0 # tensor([1])\n",
            "  v15 = torch._ops.aten.lift_fresh_copy.default(v14)\n",
            "  v16 = torch._ops.aten.cumsum.default(v15,int(0))\n",
            "  v17 = torch._ops.aten.slice.Tensor(v16,int(0),int(0),int(-1))\n",
            "  v18 = torch._ops.aten.neg.default(v17)\n",
            "  v19 = torch._ops.aten.unbind.int(v18)\n",
            "  v20 = torch._ops.aten.new_zeros.default(v13,[1, 1][<class 'torch.fx.immutable_collections.immutable_list'>])\n",
            "  v21 = torch._ops.aten.diagonal.default(v20)\n",
            "  v22 = torch._ops.aten.fill_.Scalar(v21,int(1))\n",
            "  v23 = torch._ops.aten.view.default(v20,[1][<class 'torch.fx.immutable_collections.immutable_list'>])\n",
            "  v24 = torch._ops.aten.cos.default(v12)\n",
            "  v25 = torch._ops.aten.select.int(v23,int(0),int(0))\n",
            "  v26 = torch._ops.aten.mul.Tensor(v23,v24)\n",
            "  v27 = torch._ops.aten.zeros.default([15][<class 'torch.fx.immutable_collections.immutable_list'>])\n",
            "  v28 = torch._ops.aten.arange.start_step(int(0),int(15),int(6))\n",
            "  v29 = torch._ops.aten.expand.default(v27,[1, 15][<class 'torch.fx.immutable_collections.immutable_list'>])\n",
            "  v30 = torch._ops.aten.view.default(v26,[1, 1][<class 'torch.fx.immutable_collections.immutable_list'>])\n",
            "  v31 = torch._ops.aten.index_put.default(v29,[None, arange][<class 'torch.fx.immutable_collections.immutable_list'>],v30)\n",
            "  v32 = torch._ops.aten.view.default(v31,[1, 3, 5][<class 'torch.fx.immutable_collections.immutable_list'>])\n",
            "  v33 = torch._ops.aten.add.Tensor(v32,v32)\n",
            "  v34 = torch._ops.aten.split_with_sizes.default(v33,[1][<class 'torch.fx.immutable_collections.immutable_list'>])\n",
            "  v35 = getitem(v34,int(0))\n",
            "  v36 = torch._ops.aten.view.default(v35,[3, 5][<class 'torch.fx.immutable_collections.immutable_list'>])\n",
            "  return v36\n"
          ]
        }
      ],
      "source": [
        "import functorch\n",
        "\n",
        "grad_f = functorch.make_fx(functorch.jacrev(f))(x)\n",
        "fx_print(grad_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMk6IojwKPOoEtWLs82Civ+",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 ('pytorch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "28f0c4cf9ff11e7fed1023153201d84a0ac9765962dc65ba2242229f995562f8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
